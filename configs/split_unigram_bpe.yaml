path: {
  data_dir: "./dataset/en-ko/",
  corpus_name: {
    "src": "aihub_en.txt", 
    "tgt": "aihub_ko.txt",
  },
  output_path: "./tokenizers/unigram_bpe/",
}

tokenizer: {
  vocab_size: [
    [2000, 2000], [2000, 4000], [2000, 8000], [2000, 16000],
    [4000, 2000], [4000, 4000], [4000, 8000], [4000, 16000],
    [8000, 2000], [8000, 4000], [8000, 8000], [8000, 16000],
    [16000, 2000], [16000, 4000], [16000, 8000], [16000, 16000]
  ],
  vocab_type: {
    "src": "unigram", 
    "tgt": "bpe",
  },
  args:{
    num_threads: 32,
  },
}
