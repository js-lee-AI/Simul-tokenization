path: {
  data_dir: "./dataset/en-ko",
  corpus_name: "aihub_ko.txt",
  output_path: "./tokenizers/unigram_bpe",
}

tokenizer: {
  vocab_languages: "ko",
  vocab_size: 16000,
  vocab_type: "bpe",
  max_sentence_length: 999999,
  args:{
    num_threads: 32,
  },
}