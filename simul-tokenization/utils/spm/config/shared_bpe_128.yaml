path: {
  data_dir: "../../../simul-tokenization/dataset/en-ko/",
  corpus_name: "aihub_total.txt"
}

tokenizer: {
  prefix: "shared_bpe_128k",
  vocab_size: 128000,
  vocab_type: "bpe",
  args:{
    num_threads: 32,
  },
}